{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27395370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnschool/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johnschool/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dc1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(filename):\n",
    "    data = {}\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"JSON data loaded successfully:\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return {\"data\": data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79929de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data loaded successfully:\n",
      "JSON data loaded successfully:\n"
     ]
    }
   ],
   "source": [
    "node_new_Twibot22 = get_file(\"data/Twibot-22/node_new.json\")\n",
    "label_new_Twibot22 = get_file(\"data/Twibot-22/label_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point1 = node_new_Twibot22[\"data\"][\"t1498018021658431488\"]\n",
    "point1_author_id = point1[\"author_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4e11c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20441260"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point1_author_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79355118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attachments': None, 'author_id': 20441260, 'context_annotations': None, 'conversation_id': 1498018021658431488, 'created_at': '2022-02-27 19:31:42+00:00', 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'rhpsia', 'name': 'Sia At The Game', 'id': 809332867, 'id_str': '809332867', 'indices': [3, 10]}], 'urls': []}, 'geo': None, 'id': 't1498018021658431488', 'in_reply_to_user_id': None, 'lang': 'en', 'possibly_sensitive': False, 'public_metrics': {'retweet_count': 10, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'referenced_tweets': None, 'reply_settings': None, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'text': 'RT @rhpsia: BREAKING: Telecom giant Ericsson used slush funds, trips for Iraqi officials &amp; payoffs through middlemen to executives ‚Äî and po‚Ä¶', 'withheld': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'human'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tweet\n",
    "print(point1)\n",
    "# is the author at key \"u\" + author_id a bot or not? \n",
    "label_new_Twibot22[\"data\"][\"u\"+str(point1_author_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77853b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31228 keys starting with 't' in Twibot-22/node_new.json\n",
      "Scanned 34936 top-level keys in the node object.\n",
      "Sample keys (up to 10): ['t1498018021658431488', 't1497947823005962245', 't1497902014763786241', 't1497736000008867841', 't1497640323631222784', 't1497417782450864130', 't1497402632301854722', 't1497288192969654273', 't1497277930623361033', 't1497275497126019076']\n"
     ]
    }
   ],
   "source": [
    "# Count top-level keys in data/Twibot-22/node_new.json that start with 't'\n",
    "try:\n",
    "    # node_new_Twibot22 is expected to be a dict returned by get_file() earlier in this notebook\n",
    "    data_obj = node_new_Twibot22 if not isinstance(node_new_Twibot22, dict) else node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "except NameError:\n",
    "    # If the variable isn't defined (cell not run), load the file now\n",
    "    print(\"node_new_Twibot22 not found in the notebook state - loading from file\")\n",
    "    node_new_Twibot22 = get_file(\"data/Twibot-22/node_new.json\")\n",
    "    data_obj = node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "\n",
    "# Ensure we have a mapping-like object\n",
    "if not isinstance(data_obj, dict):\n",
    "    print(\"Unexpected format: expected a JSON object/dict for node data.\")\n",
    "else:\n",
    "    keys = [k for k in data_obj.keys() if isinstance(k, str) and k.startswith(\"t\")]\n",
    "    count = len(keys)\n",
    "    print(f\"Found {count} keys starting with 't' in data/Twibot-22/node_new.json\")\n",
    "    print(f\"Scanned {len(data_obj)} top-level keys in the node object.\")\n",
    "    # Show up to 10 sample keys to verify\n",
    "    print(\"Sample keys (up to 10):\", keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8d9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8171\n",
      "23057\n",
      "644\n"
     ]
    }
   ],
   "source": [
    "# add a label to every tweet key\n",
    "num_bot = 0 \n",
    "num_human = 0\n",
    "num_unlabeled = 0\n",
    "\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    \n",
    "    if \"u\"+str(cur_author_id) in label_new_Twibot22[\"data\"]: \n",
    "        cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    else:\n",
    "        num_unlabeled+= 1 # ids nonexistent in labels\n",
    "\n",
    "    if (cur_author_label == \"human\"):\n",
    "        num_human += 1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        num_bot += 1\n",
    "\n",
    "    node_new_Twibot22[\"data\"][cur_tweet_id][\"author_label\"] = cur_author_label\n",
    "\n",
    "print(num_bot)\n",
    "print(num_human)\n",
    "print(num_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578dc13",
   "metadata": {},
   "source": [
    "There are 8171 bot tweets, and 23057 human tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34b74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for John Ryu because he is a meanie\n",
    "complete_dict = {}\n",
    "\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    \n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]:\n",
    "        continue\n",
    "    \n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    text = cur_tweet[\"text\"]\n",
    "    \n",
    "    complete_dict[cur_tweet_id] = {\n",
    "        \"label\": cur_author_label,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "with open(\"data/tweets_with_labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(complete_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8505995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(preserve_case =False)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops.update(string.punctuation)\n",
    "set_1 = {'‚Ä¶', '‚Äô', '...', '‚Äú', '‚Äù','‚Äò'}\n",
    "stops.update(set_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c489fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', '@rhpsia', ':', 'breaking', ':', 'telecom', 'giant', 'ericsson', 'used', 'slush', 'funds', ',', 'trips', 'for', 'iraqi', 'officials', '&', 'payoffs', 'through', 'middlemen', 'to', 'executives', '‚Äî', 'and', 'po', '‚Ä¶']\n",
      "Top 100 bot tokens:\n",
      "[('rt', 2695), ('new', 366), ('today', 331), ('like', 281), ('us', 259), ('get', 257), ('one', 254), ('de', 239), ('#zerotrust', 238), ('2021', 210), ('3', 206), ('Ô∏è', 205), ('punk', 201), ('time', 200), ('dos', 198), ('price', 196), ('üí¢', 195), ('„Äê', 187), ('@snaptadc', 172), ('people', 164), ('üî•', 164), ('join', 163), ('check', 158), ('2022', 158), ('„Äë', 157), ('great', 156), ('la', 156), ('#cryptoart', 154), ('know', 152), ('first', 150), ('make', 147), ('good', 142), ('#cybersecurity', 142), ('day', 139), ('trust', 139), ('#nft', 137), ('year', 136), ('pips', 134), ('love', 133), ('1', 129), ('see', 128), ('#ztna', 128), ('work', 126), ('follow', 125), ('zero', 124), ('#nftgiveaway', 124), ('please', 123), ('security', 122), ('win', 122), ('#nftcollector', 122), ('#nftartist', 121), ('#nftproject', 121), ('#nftdrop', 121), ('would', 120), ('marketing', 120), ('#nftcommumity', 120), ('#tezos', 120), ('life', 119), ('2', 118), ('üÜï', 115), ('#eurusd', 114), ('want', 113), ('think', 113), ('learn', 112), ('\\u2063', 111), ('need', 110), ('help', 108), ('world', 108), ('#forex', 108), ('#trading', 108), ('en', 108), ('el', 108), ('u', 107), ('#signaltrading', 107), ('#foreignexchange', 107), ('go', 106), ('click', 105), ('simple', 105), ('sale', 105), ('#digitalartist', 105), ('access', 103), ('5', 102), ('lucky', 102), ('best', 101), ('via', 101), ('also', 100), (':/', 100), ('back', 98), ('@banyansecurity', 98), ('‡•§', 97), ('many', 97), ('#maxcapacity', 96), ('steps', 95), ('iphone', 94), ('submit', 94), ('‡§ï‡•á', 93), ('link', 93), ('‡§Æ‡•á‡§Ç', 92), ('üòç', 92), ('#devops', 90)]\n",
      "\n",
      "Top 100 human tokens:\n",
      "[('rt', 7573), ('de', 3146), ('la', 1102), ('new', 971), ('us', 940), ('Ô∏è', 925), ('en', 883), ('one', 818), ('people', 777), ('que', 694), ('get', 682), ('üòÇ', 649), ('el', 645), ('today', 631), ('time', 619), ('like', 613), ('thanks', 589), ('day', 568), ('know', 516), ('need', 510), ('please', 459), ('thank', 452), ('work', 448), ('help', 447), ('good', 437), ('great', 436), ('üî•', 431), ('year', 430), ('first', 428), ('see', 424), ('health', 415), ('read', 404), ('los', 392), ('make', 391), ('2', 388), ('1', 386), ('love', 385), ('‚Äî', 384), ('research', 377), ('back', 369), ('would', 368), ('covid', 353), ('del', 353), ('check', 352), ('many', 349), ('right', 339), ('las', 338), ('para', 338), ('happy', 334), ('‚ù§', 331), ('years', 330), ('van', 321), ('por', 314), ('still', 310), ('3', 308), ('also', 307), ('best', 306), ('last', 304), ('go', 303), ('se', 303), ('care', 301), ('think', 289), ('hi', 289), ('take', 287), ('let', 286), ('want', 285), ('may', 284), ('world', 284), ('much', 282), ('way', 277), ('better', 276), ('support', 274), ('week', 273), ('het', 271), ('dr', 268), ('üíû', 267), ('could', 262), ('live', 254), ('got', 253), ('e', 250), ('‡§ï‡•á', 250), ('well', 249), ('es', 249), ('full', 248), ('even', 248), ('life', 244), ('really', 243), ('every', 242), ('next', 242), ('going', 242), ('te', 242), ('‡§Æ‡•á‡§Ç', 242), ('look', 240), ('op', 236), ('join', 232), ('team', 231), ('19', 231), ('find', 230), ('@senschumer', 230), ('con', 229)]\n"
     ]
    }
   ],
   "source": [
    "print(tknzr.tokenize(node_new_Twibot22[\"data\"][keys[0]][\"text\"]))\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4427bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human tweet length stats:\n",
      "Mean: 24.156702777406267\n",
      "Standard deviation: 14.07049034910416\n",
      "Median (Q2): 24.0\n",
      "Minimum: 1.0\n",
      "Maximum: 89.0\n",
      "Bot tweet length stats:\n",
      "Mean: 22.005519440696677\n",
      "Standard deviation: 12.500826670200132\n",
      "Median (Q2): 21.0\n",
      "Minimum: 1.0\n",
      "Maximum: 77.0\n"
     ]
    }
   ],
   "source": [
    "bot_lengths = np.array([])\n",
    "human_lengths = np.array([])\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    if (cur_author_label == \"human\"):\n",
    "        human_lengths = np.append(human_lengths, len(tokenized_tweet))\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        bot_lengths = np.append(bot_lengths, len(tokenized_tweet))\n",
    "def stats(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    q2 = np.percentile(arr, 50) \n",
    "    minimum = np.min(arr)\n",
    "    maximum = np.max(arr)\n",
    "\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Standard deviation:\", std)\n",
    "    print(\"Median (Q2):\", q2)\n",
    "    print(\"Minimum:\", minimum)\n",
    "    print(\"Maximum:\", maximum)\n",
    "\n",
    "print(\"Human tweet length stats:\")\n",
    "stats(human_lengths)\n",
    "print(\"Bot tweet length stats:\")\n",
    "stats(bot_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b13c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 bot tokens:\n",
      "[(('dos', 'punk'), 188), (('rt', '@snaptadc'), 169), (('#nftcommumity', '#nftgiveaway'), 120), (('#nftdrop', '#nftartist'), 120), (('#nftartist', '#nftcollector'), 120), (('#nftcollector', '#nftproject'), 120), (('zero', 'trust'), 117), (('#forex', '#signaltrading'), 107), (('#signaltrading', '#trading'), 107), (('#trading', '#eurusd'), 107), (('#eurusd', '#foreignexchange'), 107), (('simple', 'steps'), 88), (('get', 'iphone'), 87), (('iphone', 'xs'), 87), (('xs', 'today'), 87), (('today', 'follow'), 87), (('follow', 'simple'), 87), (('steps', 'win'), 87), (('win', 'click'), 87), (('click', 'submit'), 87), (('submit', 'e-mail'), 87), (('e-mail', 'lucky'), 87), (('lucky', 'winner.https'), 87), (('winner.https', ':/'), 87), (('#nftart', '#cryptoart'), 84), (('join', 'us'), 82), (('#nftgiveaway', '#nfts'), 71), (('price', 'üòä'), 70), (('üòä', '3'), 70), (('3', 'tezos'), 70), (('tezos', 'üåº'), 70), (('üåº', '#nftcommumity'), 70), (('#nfts', '#tezos'), 70), (('#tezos', '#nftdrop'), 70), (('#nftproject', '#rarible'), 70), (('#rarible', '#object'), 70), (('#object', '#nftart'), 70), (('#cryptoart', '#cryptoartist'), 70), (('#cryptoartist', '#nftphotography'), 70), (('#nftphotography', '#digitalart'), 70), (('#digitalart', '#digitalartist'), 70), (('#digitalartist', '#cryptocurrency'), 70), (('#cryptocurrency', '#nftcreator'), 70), (('#nftcreator', '#nftcreators'), 70), (('https://t.co/fhSnwgNVAO', 'price'), 69), (('pips', 'total'), 67), (('total', 'today'), 67), (('pips', '#forex'), 67), (('üí¢', 'dewi'), 65), (('dewi', 'massage'), 65), (('massage', 'therapy'), 65), (('therapy', 'üí¢'), 65), (('üí¢', 'üèÅ'), 65), (('üèÅ', 'area'), 65), (('area', 'jogja'), 65), (('jogja', 'seturan'), 65), (('üí¢', 'info'), 65), (('‚òé', 'wa'), 65), (('#pijat', '#pijatjogja'), 65), (('#pijatjogja', '#massagejogja'), 65), (('#massagejogja', '#pijatcapek'), 65), (('#pijatcapek', '#massage'), 65), (('#massage', '#recomendedmassage'), 65), (('#recomendedmassage', '#pijatsehat'), 65), (('#pijatsehat', '#jogja'), 65), (('#jogja', '#yogyakarta'), 65), (('#yogyakarta', '#recomended'), 65), (('#recomended', '#pijatenak'), 65), (('#pijatenak', '#jogjamassage'), 65), (('#jogjamassage', '#pijatvitalitasjogja'), 65), (('seturan', 'üí¢'), 64), (('info', 'rate'), 64), (('rate', 'rules'), 64), (('rules', '‚òé'), 64), (('üÜï', '„Äê'), 64), (('wa', 'https://t.co/s4ght9odTc*'), 63), (('https://t.co/s4ght9odTc*', '#pijat'), 63), (('like', 'https://t.co/7hAuMonHwn'), 60), (('\\u2063', '\\u2063'), 57), (('#iphone', '#iphonexs'), 54), (('#iphonexs', '#plus'), 54), (('#plus', '#iphonex'), 54), (('#iphonex', '#pro'), 54), (('#pro', '#iphonexsmax'), 54), (('#iphonexsmax', '#iphonexr'), 54), (('#iphonexr', '#promax'), 54), (('#promax', '#apple'), 54), (('#apple', '#appleiphone'), 54), (('üí•', 'new'), 53), (('new', 'listing'), 53), (('please', 'check'), 52), (('üî•', 'üî•'), 51), (('#planetsandbox', 'psb'), 50), (('available', '@objktcom'), 50), (('@objktcom', 'title'), 50), (('price', '3'), 50), (('3', '#tezos'), 50), (('#tezos', 'please'), 50), (('check', 'üëÄ'), 50), (('üëÄ', 'link'), 50)]\n",
      "\n",
      "Top 100 human tokens:\n",
      "[(('‚ù§', 'Ô∏è'), 292), (('üòÇ', 'üòÇ'), 255), (('de', 'la'), 250), (('üíû', 'üî•'), 219), (('rt', '@senschumer'), 212), (('üî•', 'üíû'), 212), (('covid', '19'), 164), (('de', 'los'), 153), (('rt', '@qrpoficial'), 144), (('de', 'las'), 127), (('en', 'la'), 120), (('van', 'de'), 117), (('op', 'de'), 100), (('let', 'us'), 94), (('a√±os', 'del'), 93), (('bipartisan', 'infrastructure'), 91), (('full', 'name'), 89), (('posted', 'photo'), 88), (('date', 'birth'), 88), (('a√±os', 'de'), 86), (('uno', 'de'), 85), (('agoura', 'hills'), 84), (('la', 'historia'), 83), (('us', 'know'), 82), (('join', 'us'), 82), (('‡§ú‡§ø‡§≤‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä', '#‡§™‡•ç‡§∞‡§ø‡§Ø‡§Ç‡§ï‡§æ_‡§®‡§ø‡§∞‡§Ç‡§ú‡§®'), 75), (('en', 'el'), 72), (('president', 'biden'), 70), (('una', 'de'), 69), (('health', 'care'), 68), (('public', 'health'), 64), (('name', 'account'), 64), (('looking', 'forward'), 63), (('account', 'number'), 63), (('üü©', 'üü©'), 61), (('infrastructure', 'law'), 59), (('years', 'ago'), 59), (('holiday', 'lakes'), 59), (('lakes', 'ohio'), 59), (('email', 'address'), 58), (('‚ñë', '‚ñë'), 57), (('voting', 'rights'), 56), (('posted', 'video'), 56), (('listen', 'announcements'), 54), (('üî•', 'üî•'), 53), (('que', 'se'), 52), (('supreme', 'court'), 51), (('looks', 'like'), 51), (('rt', '@conversationedu'), 50), (('address', 'thanks'), 50), (('happy', 'birthday'), 48), (('make', 'sure'), 48), (('ü§£', 'ü§£'), 47), (('pure', 'silk'), 47), (('lo', 'que'), 46), (('build', 'back'), 46), (('kindly', 'dm'), 46), (('last', 'week'), 45), (('‚ô•', 'Ô∏è'), 45), (('rt', '@surgeon_general'), 45), (('rt', '@senatedems'), 45), (('vocalista', 'de'), 45), (('rt', '@advicetowriters'), 45), (('new', 'research'), 44), (('et', 'al'), 44), (('del', 'rock'), 44), (('new', 'year'), 44), (('voor', 'de'), 44), (('dm', 'us'), 43), (('#dotarcade', '#metaverse'), 43), (('surgeon', 'general'), 42), (('#metaverse', '#rts'), 42), (('lanzamiento', 'de'), 42), (('con', 'el'), 41), (('back', 'better'), 41), (('sorry', 'hear'), 41), (('por', 'favor'), 41), (('se', 'cumplieron'), 41), (('congratulations', '@triimdphd'), 41), (('rt', '@mizdanaclaire'), 41), (('buses', 'replace'), 40), (('replace', 'trains'), 40), (('‚Äº', 'Ô∏è'), 40), (('del', 'lanzamiento'), 40), (('every', 'day'), 39), (('first', 'time'), 39), (('üò≠', 'üò≠'), 38), (('üòÅ', 'üòÅ'), 38), (('rt', '@feyi_x'), 38), (('nice', 'day'), 38), (('de', 'uno'), 38), (('Ô∏è', '‚ù§'), 37), (('rt', '@michaelworobey'), 37), (('‚¨ú', 'üü©'), 37), (('tell', 'us'), 37), (('#hapc', '#medtwitter'), 37), (('right', 'vote'), 37), (('hoy', 'se'), 37), (('check', 'information'), 36), (('information', 'displays'), 36)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    filtered_tweet = bigrams(filtered_tweet)\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2801652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 bot tokens:\n",
      "[(('#nftdrop', '#nftartist', '#nftcollector'), 120), (('#nftartist', '#nftcollector', '#nftproject'), 120), (('#forex', '#signaltrading', '#trading'), 107), (('#signaltrading', '#trading', '#eurusd'), 107), (('#trading', '#eurusd', '#foreignexchange'), 107), (('get', 'iphone', 'xs'), 87), (('iphone', 'xs', 'today'), 87), (('xs', 'today', 'follow'), 87), (('today', 'follow', 'simple'), 87), (('follow', 'simple', 'steps'), 87), (('simple', 'steps', 'win'), 87), (('steps', 'win', 'click'), 87), (('win', 'click', 'submit'), 87), (('click', 'submit', 'e-mail'), 87), (('submit', 'e-mail', 'lucky'), 87), (('e-mail', 'lucky', 'winner.https'), 87), (('lucky', 'winner.https', ':/'), 87), (('price', 'üòä', '3'), 70), (('üòä', '3', 'tezos'), 70), (('3', 'tezos', 'üåº'), 70), (('tezos', 'üåº', '#nftcommumity'), 70), (('üåº', '#nftcommumity', '#nftgiveaway'), 70), (('#nftcommumity', '#nftgiveaway', '#nfts'), 70), (('#nftgiveaway', '#nfts', '#tezos'), 70), (('#nfts', '#tezos', '#nftdrop'), 70), (('#tezos', '#nftdrop', '#nftartist'), 70), (('#nftcollector', '#nftproject', '#rarible'), 70), (('#nftproject', '#rarible', '#object'), 70), (('#rarible', '#object', '#nftart'), 70), (('#object', '#nftart', '#cryptoart'), 70), (('#nftart', '#cryptoart', '#cryptoartist'), 70), (('#cryptoart', '#cryptoartist', '#nftphotography'), 70), (('#cryptoartist', '#nftphotography', '#digitalart'), 70), (('#nftphotography', '#digitalart', '#digitalartist'), 70), (('#digitalart', '#digitalartist', '#cryptocurrency'), 70), (('#digitalartist', '#cryptocurrency', '#nftcreator'), 70), (('#cryptocurrency', '#nftcreator', '#nftcreators'), 70), (('https://t.co/fhSnwgNVAO', 'price', 'üòä'), 69), (('pips', 'total', 'today'), 67), (('pips', '#forex', '#signaltrading'), 67), (('üí¢', 'dewi', 'massage'), 65), (('dewi', 'massage', 'therapy'), 65), (('massage', 'therapy', 'üí¢'), 65), (('therapy', 'üí¢', 'üèÅ'), 65), (('üí¢', 'üèÅ', 'area'), 65), (('üèÅ', 'area', 'jogja'), 65), (('area', 'jogja', 'seturan'), 65), (('#pijat', '#pijatjogja', '#massagejogja'), 65), (('#pijatjogja', '#massagejogja', '#pijatcapek'), 65), (('#massagejogja', '#pijatcapek', '#massage'), 65), (('#pijatcapek', '#massage', '#recomendedmassage'), 65), (('#massage', '#recomendedmassage', '#pijatsehat'), 65), (('#recomendedmassage', '#pijatsehat', '#jogja'), 65), (('#pijatsehat', '#jogja', '#yogyakarta'), 65), (('#jogja', '#yogyakarta', '#recomended'), 65), (('#yogyakarta', '#recomended', '#pijatenak'), 65), (('#recomended', '#pijatenak', '#jogjamassage'), 65), (('#pijatenak', '#jogjamassage', '#pijatvitalitasjogja'), 65), (('jogja', 'seturan', 'üí¢'), 64), (('seturan', 'üí¢', 'info'), 64), (('üí¢', 'info', 'rate'), 64), (('info', 'rate', 'rules'), 64), (('rate', 'rules', '‚òé'), 64), (('rules', '‚òé', 'wa'), 64), (('‚òé', 'wa', 'https://t.co/s4ght9odTc*'), 63), (('wa', 'https://t.co/s4ght9odTc*', '#pijat'), 63), (('https://t.co/s4ght9odTc*', '#pijat', '#pijatjogja'), 63), (('#iphone', '#iphonexs', '#plus'), 54), (('#iphonexs', '#plus', '#iphonex'), 54), (('#plus', '#iphonex', '#pro'), 54), (('#iphonex', '#pro', '#iphonexsmax'), 54), (('#pro', '#iphonexsmax', '#iphonexr'), 54), (('#iphonexsmax', '#iphonexr', '#promax'), 54), (('#iphonexr', '#promax', '#apple'), 54), (('#promax', '#apple', '#appleiphone'), 54), (('üí•', 'new', 'listing'), 53), (('available', '@objktcom', 'title'), 50), (('price', '3', '#tezos'), 50), (('3', '#tezos', 'please'), 50), (('#tezos', 'please', 'check'), 50), (('please', 'check', 'üëÄ'), 50), (('check', 'üëÄ', 'link'), 50), (('winner.https', ':/', 't.co/rqsaub5opp'), 49), (('@planetsandbox', '#planetsandbox', 'psb'), 44), (('#planetsandbox', 'psb', '#bsc'), 44), (('psb', '#bsc', '#gamefi'), 44), (('@kissingercenter', '@saishopkins', 'prof'), 39), (('#ads', '#googleads', '#datascience'), 39), (('#googleads', '#datascience', '#digitalmarketingagency'), 39), (('#datascience', '#digitalmarketingagency', '#marketing'), 39), (('#digitalmarketingagency', '#marketing', '#marketingchampions'), 39), (('#marketing', '#marketingchampions', '#marketingdigital'), 39), (('#marketingchampions', '#marketingdigital', '#marketingforthenow'), 39), (('#marketingdigital', '#marketingforthenow', '#b2c'), 39), (('#marketingforthenow', '#b2c', '#branding'), 39), (('#b2c', '#branding', '#brandidentity'), 39), (('#branding', '#brandidentity', '#digitalmarketingmonth'), 39), (('#brandidentity', '#digitalmarketingmonth', '#advertising'), 39), (('#adtech', '#ads', '#googleads'), 38), (('„Äë', '„Éà„Éº„ÇØ„Éú„Ç§„Çπ', 'üí¨'), 37)]\n",
      "\n",
      "Top 100 human tokens:\n",
      "[(('üíû', 'üî•', 'üíû'), 196), (('üî•', 'üíû', 'üî•'), 186), (('uno', 'de', 'los'), 84), (('üòÇ', 'üòÇ', 'üòÇ'), 73), (('let', 'us', 'know'), 71), (('una', 'de', 'las'), 68), (('full', 'name', 'account'), 63), (('name', 'account', 'number'), 62), (('holiday', 'lakes', 'ohio'), 59), (('bipartisan', 'infrastructure', 'law'), 51), (('de', 'la', 'historia'), 46), (('#dotarcade', '#metaverse', '#rts'), 42), (('build', 'back', 'better'), 41), (('buses', 'replace', 'trains'), 40), (('a√±os', 'del', 'lanzamiento'), 39), (('de', 'uno', 'de'), 38), (('Ô∏è', '‚ù§', 'Ô∏è'), 37), (('check', 'information', 'displays'), 36), (('üü©', 'üü©', 'üü©'), 36), (('‚ù§', 'Ô∏è', '‚ù§'), 36), (('‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç', '‡Æú‡ØÜ‡ÆØ‡Ææ', '‡Æü‡Æø‡Æµ‡Æø‡ÆØ‡Æø‡Æ≤‡Øç'), 35), (('information', 'displays', 'listen'), 34), (('displays', 'listen', 'announcements'), 34), (('‡§ú‡§ø‡§≤‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä', '#‡§™‡•ç‡§∞‡§ø‡§Ø‡§Ç‡§ï‡§æ_‡§®‡§ø‡§∞‡§Ç‡§ú‡§®', '‡§ï‡•á'), 34), (('date', 'birth', 'thanks'), 34), (('del', 'lanzamiento', 'de'), 34), (('‚ô•', 'Ô∏è', '‚ô•'), 33), (('Ô∏è', '‚ô•', 'Ô∏è'), 33), (('#‡§™‡•ç‡§∞‡§ø‡§Ø‡§Ç‡§ï‡§æ_‡§®‡§ø‡§∞‡§Ç‡§ú‡§®', '‡§ï‡•á', '‡§®‡•á‡§§‡•É‡§§‡•ç‡§µ'), 32), (('posted', 'video', 'holiday'), 32), (('video', 'holiday', 'lakes'), 32), (('line', 'buses', 'replace'), 31), (('american', 'rescue', 'plan'), 31), (('‡§ï‡•á', '‡§®‡•á‡§§‡•É‡§§‡•ç‡§µ', '‡§Æ‡•á‡§Ç'), 31), (('address', 'date', 'birth'), 31), (('home', 'address', 'thanks'), 31), (('works', 'take', 'place'), 30), (('en', 'la', 'historia'), 30), (('hoy', 'se', 'cumplen'), 30), (('‡ÆÆ‡Æ£‡Æø‡Æï‡Øç‡Æï‡ØÅ', '‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç', '‡Æú‡ØÜ‡ÆØ‡Ææ'), 30), (('la', 'historia', 'del'), 29), (('‚ù§', 'Ô∏è', 'üôèüèΩ'), 28), (('historia', 'del', 'rock'), 28), (('city', 'agoura', 'hills'), 27), (('kindly', 'dm', 'us'), 27), (('number', 'date', 'birth'), 27), (('name', 'date', 'birth'), 27), (('de', 'los', 'grandes'), 27), (('martin', 'luther', 'king'), 26), (('please', 'tell', 'us'), 26), (('luther', 'king', 'jr'), 25), (('movement', 'lab', 'ohio'), 25), (('rio', 'de', 'janeiro'), 25), (('first', 'human', 'mars'), 25), (('mars', '.\\n.', '@nerocosmos'), 25), (('.\\n.', '@nerocosmos', 'x'), 25), (('@nerocosmos', 'x', 'soulengineer'), 25), (('x', 'soulengineer', 'collab'), 25), (('soulengineer', 'collab', '.\\n.'), 25), (('daily', '#photooftheday', '#france'), 24), (('line', 'major', 'delays'), 24), (('üôèüèΩ', '‚ù§', 'Ô∏è'), 24), (('date', 'birth', 'email'), 24), (('birth', 'email', 'address'), 24), (('bipartisan', 'infrastructure', 'bill'), 23), (('üè¥', '\\U000e0067', '\\U000e0062'), 23), (('‚ñë', '‚ñë', '‚ñë'), 23), (('send', 'us', 'dm'), 23), (('#cityofagourahills', '#agourahills', '#agouracares'), 23), (('de', 'los', 'mejores'), 23), (('#cosmonaut', '#spaceman', '#mars'), 23), (('‚¨ú', 'üü©', 'üü©'), 22), (('major', 'delays', 'due'), 22), (('altered', 'short', 'notice'), 22), (('üî•', 'üî•', 'üî•'), 22), (('‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ', '‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø', '‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®'), 22), (('us', 'know', 'goes'), 22), (('email', 'address', 'date'), 22), (('#spaceman', '#mars', '#redplanet'), 22), (('‚¨ú', 'Ô∏è', '‚¨ú'), 22), (('Ô∏è', '‚¨ú', 'Ô∏è'), 22), (('happy', 'new', 'year'), 21), (('dr', 'martin', 'luther'), 21), (('congratulations', '@triimdphd', 'alum'), 21), (('posted', 'photo', 'holiday'), 21), (('photo', 'holiday', 'lakes'), 21), (('services', 'may', 'altered'), 20), (('may', 'altered', 'short'), 20), (('ü§£', 'ü§£', 'ü§£'), 20), (('a√±os', 'de', 'uno'), 20), (('‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø', '‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®', '2022'), 20), (('‡§ú‡§ø‡§≤‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞‡•Ä', '#‡§™‡•ç‡§∞‡§ø‡§Ø‡§Ç‡§ï‡§æ_‡§®‡§ø‡§∞‡§Ç‡§ú‡§®', '‡§®‡•á'), 20), (('#agourahills', '#agouracares', '#conejovalley'), 20), (('may', 'please', 'tell'), 20), (('sorry', 'hear', 'kindly'), 20), (('account', 'number', 'email'), 20), (('number', 'email', 'address'), 20), (('birth', 'thanks', 'laura'), 20), (('look', 'full', 'name'), 20), (('¬°', 'feliz', 'cumplea√±os'), 20)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    filtered_tweet = trigrams(filtered_tweet)\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
