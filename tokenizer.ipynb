{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27395370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(filename):\n",
    "    data = {}\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"JSON data loaded successfully:\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return {\"data\": data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79929de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_new_Twibot22 = get_file(\"Twibot-22/node_new.json\")\n",
    "label_new_Twibot22 = get_file(\"Twibot-22/label_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point1 = node_new_Twibot22[\"data\"][\"t1498018021658431488\"]\n",
    "point1_author_id = point1[\"author_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79355118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tweet\n",
    "print(point1)\n",
    "# is the author at key \"u\" + author_id a bot or not? \n",
    "label_new_Twibot22[\"data\"][\"u\"+str(point1_author_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77853b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top-level keys in Twibot-22/node_new.json that start with 't'\n",
    "try:\n",
    "    # node_new_Twibot22 is expected to be a dict returned by get_file() earlier in this notebook\n",
    "    data_obj = node_new_Twibot22 if not isinstance(node_new_Twibot22, dict) else node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "except NameError:\n",
    "    # If the variable isn't defined (cell not run), load the file now\n",
    "    print(\"node_new_Twibot22 not found in the notebook state - loading from file\")\n",
    "    node_new_Twibot22 = get_file(\"Twibot-22/node_new.json\")\n",
    "    data_obj = node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "\n",
    "# Ensure we have a mapping-like object\n",
    "if not isinstance(data_obj, dict):\n",
    "    print(\"Unexpected format: expected a JSON object/dict for node data.\")\n",
    "else:\n",
    "    keys = [k for k in data_obj.keys() if isinstance(k, str) and k.startswith(\"t\")]\n",
    "    count = len(keys)\n",
    "    print(f\"Found {count} keys starting with 't' in Twibot-22/node_new.json\")\n",
    "    print(f\"Scanned {len(data_obj)} top-level keys in the node object.\")\n",
    "    # Show up to 10 sample keys to verify\n",
    "    print(\"Sample keys (up to 10):\", keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a label to every tweet key\n",
    "num_bot = 0 \n",
    "num_human = 0\n",
    "num_unlabeled = 0\n",
    "\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    \n",
    "    if \"u\"+str(cur_author_id) in label_new_Twibot22[\"data\"]: \n",
    "        cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    else:\n",
    "        num_unlabeled+= 1 # ids nonexistent in labels\n",
    "\n",
    "    if (cur_author_label == \"human\"):\n",
    "        num_human += 1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        num_bot += 1\n",
    "\n",
    "    node_new_Twibot22[\"data\"][cur_tweet_id][\"author_label\"] = cur_author_label\n",
    "\n",
    "print(num_bot)\n",
    "print(num_human)\n",
    "print(num_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578dc13",
   "metadata": {},
   "source": [
    "There are 8171 bot tweets, and 23057 human tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for John Ryu because he is a meanie\n",
    "complete_dict = {}\n",
    "\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    \n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]:\n",
    "        continue\n",
    "    \n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    text = cur_tweet[\"text\"]\n",
    "    \n",
    "    complete_dict[cur_tweet_id] = {\n",
    "        \"label\": cur_author_label,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "with open(\"tweets_with_labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(complete_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8505995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(preserve_case =False)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops.update(string.punctuation)\n",
    "set_1 = {'…', '’', '...', '“', '”','‘'}\n",
    "stops.update(set_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c489fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tknzr.tokenize(node_new_Twibot22[\"data\"][keys[0]][\"text\"]))\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_lengths = np.array([])\n",
    "human_lengths = np.array([])\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    if (cur_author_label == \"human\"):\n",
    "        human_lengths = np.append(human_lengths, len(tokenized_tweet))\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        bot_lengths = np.append(bot_lengths, len(tokenized_tweet))\n",
    "def stats(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    q2 = np.percentile(arr, 50) \n",
    "    minimum = np.min(arr)\n",
    "    maximum = np.max(arr)\n",
    "\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Standard deviation:\", std)\n",
    "    print(\"Median (Q2):\", q2)\n",
    "    print(\"Minimum:\", minimum)\n",
    "    print(\"Maximum:\", maximum)\n",
    "\n",
    "print(\"Human tweet length stats:\")\n",
    "stats(human_lengths)\n",
    "print(\"Bot tweet length stats:\")\n",
    "stats(bot_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    filtered_tweet = bigrams(filtered_tweet)\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2801652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import trigrams\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    filtered_tweet = trigrams(filtered_tweet)\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
