{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27395370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dc1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(filename):\n",
    "    data = {}\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"JSON data loaded successfully:\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return {\"data\": data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79929de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data loaded successfully:\n",
      "JSON data loaded successfully:\n"
     ]
    }
   ],
   "source": [
    "node_new_Twibot22 = get_file(\"Twibot-22/node_new.json\")\n",
    "label_new_Twibot22 = get_file(\"Twibot-22/label_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point1 = node_new_Twibot22[\"data\"][\"t1498018021658431488\"]\n",
    "point1_author_id = point1[\"author_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79355118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attachments': None, 'author_id': 20441260, 'context_annotations': None, 'conversation_id': 1498018021658431488, 'created_at': '2022-02-27 19:31:42+00:00', 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'rhpsia', 'name': 'Sia At The Game', 'id': 809332867, 'id_str': '809332867', 'indices': [3, 10]}], 'urls': []}, 'geo': None, 'id': 't1498018021658431488', 'in_reply_to_user_id': None, 'lang': 'en', 'possibly_sensitive': False, 'public_metrics': {'retweet_count': 10, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'referenced_tweets': None, 'reply_settings': None, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'text': 'RT @rhpsia: BREAKING: Telecom giant Ericsson used slush funds, trips for Iraqi officials &amp; payoffs through middlemen to executives — and po…', 'withheld': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'human'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tweet\n",
    "print(point1)\n",
    "# is the author at key \"u\" + author_id a bot or not? \n",
    "label_new_Twibot22[\"data\"][\"u\"+str(point1_author_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77853b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31228 keys starting with 't' in Twibot-22/node_new.json\n",
      "Scanned 34936 top-level keys in the node object.\n",
      "Sample keys (up to 10): ['t1498018021658431488', 't1497947823005962245', 't1497902014763786241', 't1497736000008867841', 't1497640323631222784', 't1497417782450864130', 't1497402632301854722', 't1497288192969654273', 't1497277930623361033', 't1497275497126019076']\n"
     ]
    }
   ],
   "source": [
    "# Count top-level keys in Twibot-22/node_new.json that start with 't'\n",
    "try:\n",
    "    # node_new_Twibot22 is expected to be a dict returned by get_file() earlier in this notebook\n",
    "    data_obj = node_new_Twibot22 if not isinstance(node_new_Twibot22, dict) else node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "except NameError:\n",
    "    # If the variable isn't defined (cell not run), load the file now\n",
    "    print(\"node_new_Twibot22 not found in the notebook state - loading from file\")\n",
    "    node_new_Twibot22 = get_file(\"Twibot-22/node_new.json\")\n",
    "    data_obj = node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "\n",
    "# Ensure we have a mapping-like object\n",
    "if not isinstance(data_obj, dict):\n",
    "    print(\"Unexpected format: expected a JSON object/dict for node data.\")\n",
    "else:\n",
    "    keys = [k for k in data_obj.keys() if isinstance(k, str) and k.startswith(\"t\")]\n",
    "    count = len(keys)\n",
    "    print(f\"Found {count} keys starting with 't' in Twibot-22/node_new.json\")\n",
    "    print(f\"Scanned {len(data_obj)} top-level keys in the node object.\")\n",
    "    # Show up to 10 sample keys to verify\n",
    "    print(\"Sample keys (up to 10):\", keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8171\n",
      "23057\n",
      "644\n"
     ]
    }
   ],
   "source": [
    "# add a label to every tweet key\n",
    "num_bot = 0 \n",
    "num_human = 0\n",
    "num_unlabeled = 0\n",
    "\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    \n",
    "    if \"u\"+str(cur_author_id) in label_new_Twibot22[\"data\"]: \n",
    "        cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    else:\n",
    "        num_error+= 1 # ids nonexistent in labels\n",
    "\n",
    "    if (cur_author_label == \"human\"):\n",
    "        num_human += 1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        num_bot += 1\n",
    "\n",
    "    node_new_Twibot22[\"data\"][cur_tweet_id][\"author_label\"] = cur_author_label\n",
    "\n",
    "print(num_bot)\n",
    "print(num_human)\n",
    "print(num_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578dc13",
   "metadata": {},
   "source": [
    "There are 8171 bot tweets, and 23057 human tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e34b74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
