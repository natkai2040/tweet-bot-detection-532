{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27395370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericktan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76dc1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(filename):\n",
    "    data = {}\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"JSON data loaded successfully:\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return {\"data\": data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79929de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data loaded successfully:\n",
      "JSON data loaded successfully:\n"
     ]
    }
   ],
   "source": [
    "node_new_Twibot22 = get_file(\"Twibot-22/node_new.json\")\n",
    "label_new_Twibot22 = get_file(\"Twibot-22/label_new.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point1 = node_new_Twibot22[\"data\"][\"t1498018021658431488\"]\n",
    "point1_author_id = point1[\"author_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79355118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attachments': None, 'author_id': 20441260, 'context_annotations': None, 'conversation_id': 1498018021658431488, 'created_at': '2022-02-27 19:31:42+00:00', 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'rhpsia', 'name': 'Sia At The Game', 'id': 809332867, 'id_str': '809332867', 'indices': [3, 10]}], 'urls': []}, 'geo': None, 'id': 't1498018021658431488', 'in_reply_to_user_id': None, 'lang': 'en', 'possibly_sensitive': False, 'public_metrics': {'retweet_count': 10, 'reply_count': None, 'like_count': 0, 'quote_count': None}, 'referenced_tweets': None, 'reply_settings': None, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'text': 'RT @rhpsia: BREAKING: Telecom giant Ericsson used slush funds, trips for Iraqi officials &amp; payoffs through middlemen to executives ‚Äî and po‚Ä¶', 'withheld': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'human'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tweet\n",
    "print(point1)\n",
    "# is the author at key \"u\" + author_id a bot or not? \n",
    "label_new_Twibot22[\"data\"][\"u\"+str(point1_author_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77853b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31228 keys starting with 't' in Twibot-22/node_new.json\n",
      "Scanned 34936 top-level keys in the node object.\n",
      "Sample keys (up to 10): ['t1498018021658431488', 't1497947823005962245', 't1497902014763786241', 't1497736000008867841', 't1497640323631222784', 't1497417782450864130', 't1497402632301854722', 't1497288192969654273', 't1497277930623361033', 't1497275497126019076']\n"
     ]
    }
   ],
   "source": [
    "# Count top-level keys in Twibot-22/node_new.json that start with 't'\n",
    "try:\n",
    "    # node_new_Twibot22 is expected to be a dict returned by get_file() earlier in this notebook\n",
    "    data_obj = node_new_Twibot22 if not isinstance(node_new_Twibot22, dict) else node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "except NameError:\n",
    "    # If the variable isn't defined (cell not run), load the file now\n",
    "    print(\"node_new_Twibot22 not found in the notebook state - loading from file\")\n",
    "    node_new_Twibot22 = get_file(\"Twibot-22/node_new.json\")\n",
    "    data_obj = node_new_Twibot22.get(\"data\", node_new_Twibot22)\n",
    "\n",
    "# Ensure we have a mapping-like object\n",
    "if not isinstance(data_obj, dict):\n",
    "    print(\"Unexpected format: expected a JSON object/dict for node data.\")\n",
    "else:\n",
    "    keys = [k for k in data_obj.keys() if isinstance(k, str) and k.startswith(\"t\")]\n",
    "    count = len(keys)\n",
    "    print(f\"Found {count} keys starting with 't' in Twibot-22/node_new.json\")\n",
    "    print(f\"Scanned {len(data_obj)} top-level keys in the node object.\")\n",
    "    # Show up to 10 sample keys to verify\n",
    "    print(\"Sample keys (up to 10):\", keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d8d9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8171\n",
      "23057\n",
      "644\n"
     ]
    }
   ],
   "source": [
    "# add a label to every tweet key\n",
    "num_bot = 0 \n",
    "num_human = 0\n",
    "num_unlabeled = 0\n",
    "\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    \n",
    "    if \"u\"+str(cur_author_id) in label_new_Twibot22[\"data\"]: \n",
    "        cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    else:\n",
    "        num_unlabeled+= 1 # ids nonexistent in labels\n",
    "\n",
    "    if (cur_author_label == \"human\"):\n",
    "        num_human += 1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        num_bot += 1\n",
    "\n",
    "    node_new_Twibot22[\"data\"][cur_tweet_id][\"author_label\"] = cur_author_label\n",
    "\n",
    "print(num_bot)\n",
    "print(num_human)\n",
    "print(num_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578dc13",
   "metadata": {},
   "source": [
    "There are 8171 bot tweets, and 23057 human tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34b74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8505995",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(preserve_case =False)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops.update(string.punctuation)\n",
    "set_1 = {'‚Ä¶', '‚Äô', '...', '‚Äú', '‚Äù','‚Äò'}\n",
    "stops.update(set_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99c489fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', '@rhpsia', ':', 'breaking', ':', 'telecom', 'giant', 'ericsson', 'used', 'slush', 'funds', ',', 'trips', 'for', 'iraqi', 'officials', '&', 'payoffs', 'through', 'middlemen', 'to', 'executives', '‚Äî', 'and', 'po', '‚Ä¶']\n",
      "Top 100 bot tokens:\n",
      "[('rt', 2695), ('new', 366), ('today', 331), ('like', 281), ('us', 259), ('get', 257), ('one', 254), ('de', 239), ('#zerotrust', 238), ('2021', 210), ('3', 206), ('Ô∏è', 205), ('punk', 201), ('time', 200), ('dos', 198), ('price', 196), ('üí¢', 195), ('„Äê', 187), ('@snaptadc', 172), ('people', 164), ('üî•', 164), ('join', 163), ('check', 158), ('2022', 158), ('„Äë', 157), ('great', 156), ('la', 156), ('#cryptoart', 154), ('know', 152), ('first', 150), ('make', 147), ('good', 142), ('#cybersecurity', 142), ('day', 139), ('trust', 139), ('#nft', 137), ('year', 136), ('pips', 134), ('love', 133), ('1', 129), ('see', 128), ('#ztna', 128), ('work', 126), ('follow', 125), ('zero', 124), ('#nftgiveaway', 124), ('please', 123), ('security', 122), ('win', 122), ('#nftcollector', 122), ('#nftartist', 121), ('#nftproject', 121), ('#nftdrop', 121), ('would', 120), ('marketing', 120), ('#nftcommumity', 120), ('#tezos', 120), ('life', 119), ('2', 118), ('üÜï', 115), ('#eurusd', 114), ('want', 113), ('think', 113), ('learn', 112), ('\\u2063', 111), ('need', 110), ('help', 108), ('world', 108), ('#forex', 108), ('#trading', 108), ('en', 108), ('el', 108), ('u', 107), ('#signaltrading', 107), ('#foreignexchange', 107), ('go', 106), ('click', 105), ('simple', 105), ('sale', 105), ('#digitalartist', 105), ('access', 103), ('5', 102), ('lucky', 102), ('best', 101), ('via', 101), ('also', 100), (':/', 100), ('back', 98), ('@banyansecurity', 98), ('‡•§', 97), ('many', 97), ('#maxcapacity', 96), ('steps', 95), ('iphone', 94), ('submit', 94), ('‡§ï‡•á', 93), ('link', 93), ('‡§Æ‡•á‡§Ç', 92), ('üòç', 92), ('#devops', 90)]\n",
      "\n",
      "Top 100 human tokens:\n",
      "[('rt', 7573), ('de', 3146), ('la', 1102), ('new', 971), ('us', 940), ('Ô∏è', 925), ('en', 883), ('one', 818), ('people', 777), ('que', 694), ('get', 682), ('üòÇ', 649), ('el', 645), ('today', 631), ('time', 619), ('like', 613), ('thanks', 589), ('day', 568), ('know', 516), ('need', 510), ('please', 459), ('thank', 452), ('work', 448), ('help', 447), ('good', 437), ('great', 436), ('üî•', 431), ('year', 430), ('first', 428), ('see', 424), ('health', 415), ('read', 404), ('los', 392), ('make', 391), ('2', 388), ('1', 386), ('love', 385), ('‚Äî', 384), ('research', 377), ('back', 369), ('would', 368), ('covid', 353), ('del', 353), ('check', 352), ('many', 349), ('right', 339), ('las', 338), ('para', 338), ('happy', 334), ('‚ù§', 331), ('years', 330), ('van', 321), ('por', 314), ('still', 310), ('3', 308), ('also', 307), ('best', 306), ('last', 304), ('go', 303), ('se', 303), ('care', 301), ('think', 289), ('hi', 289), ('take', 287), ('let', 286), ('want', 285), ('may', 284), ('world', 284), ('much', 282), ('way', 277), ('‚Äò', 277), ('better', 276), ('support', 274), ('week', 273), ('het', 271), ('dr', 268), ('üíû', 267), ('could', 262), ('live', 254), ('got', 253), ('e', 250), ('‡§ï‡•á', 250), ('well', 249), ('es', 249), ('full', 248), ('even', 248), ('life', 244), ('really', 243), ('every', 242), ('next', 242), ('going', 242), ('te', 242), ('‡§Æ‡•á‡§Ç', 242), ('look', 240), ('op', 236), ('join', 232), ('team', 231), ('19', 231), ('find', 230), ('@senschumer', 230)]\n"
     ]
    }
   ],
   "source": [
    "print(tknzr.tokenize(node_new_Twibot22[\"data\"][keys[0]][\"text\"]))\n",
    "bot_dict = {}\n",
    "human_dict = {}\n",
    "for cur_tweet_id in keys: \n",
    "    cur_tweet = node_new_Twibot22[\"data\"][cur_tweet_id]\n",
    "    cur_author_id = cur_tweet[\"author_id\"]\n",
    "    if \"u\"+str(cur_author_id) not in label_new_Twibot22[\"data\"]: \n",
    "        continue\n",
    "    cur_author_label = label_new_Twibot22[\"data\"][\"u\"+str(cur_author_id)]\n",
    "    tokenized_tweet = tknzr.tokenize(cur_tweet[\"text\"])\n",
    "    filtered_tweet = [word for word in tokenized_tweet if word not in stops]\n",
    "    if (cur_author_label == \"human\"):\n",
    "        for token in filtered_tweet:\n",
    "            if human_dict.get(token, 0) == 0:\n",
    "                human_dict[token] = 1\n",
    "            else:\n",
    "                 human_dict[token] = human_dict[token]+1\n",
    "    if (cur_author_label == \"bot\"):\n",
    "        for token in filtered_tweet:\n",
    "            if bot_dict.get(token, 0) == 0:\n",
    "                bot_dict[token] = 1\n",
    "            else:\n",
    "                 bot_dict[token] = bot_dict[token]+1\n",
    "sorted_bot = dict(sorted(bot_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_human = dict(sorted(human_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Top 100 bot tokens:\")\n",
    "print(list(sorted_bot.items())[:100])\n",
    "print(\"\\nTop 100 human tokens:\")\n",
    "print(list(sorted_human.items())[:100])\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
